{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88XCmqiORe87"
      },
      "source": [
        ">## Assignment 1 - DSAI 544 Computer Vision With Machine Learning\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSgeK3aEMHsa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fDuka2Ua1Ov"
      },
      "source": [
        ">Student's Name: *YÄ°ÄžÄ°T ATEÅž*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKB2kpA-a1x4"
      },
      "source": [
        "* The full assignment is worth 15 points in total. Please write your answers within the correct exercise sections.\n",
        "* Please sumbit your .ipynb file to Moodle before due time.\n",
        "* Files should be named in the following format: DSAI544 Assignment 1 Name Surname\n",
        "* You must run your code, the outputs (especially from test codes) will be important in the grading process. There are somes codes to test your code, use them to test your solution, and make sure to keep those codes and their printouts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AXqZxL_quzVU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHXxqvUNzx1b",
        "outputId": "9cd222b1-f78e-45ca-fb53-c60f8aae0e56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "test_transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
        "cifar10 = torchvision.datasets.CIFAR10(root='./data', download=True, train=True, transform=transform)\n",
        "cifar10_test = torchvision.datasets.CIFAR10(root='./data', download=True, train=False, transform=test_transform)\n",
        "\n",
        "batch_size = 32\n",
        "n = batch_size\n",
        "dataloader = DataLoader(cifar10, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "dataloader_test = DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQLdDUBfjviL"
      },
      "source": [
        "## Backprop Your Own Gradients\n",
        "\n",
        "In this section you will backprop your gradients through a loss function. Below you fill find an implementation of a loss function in multiple steps, an integrated loss function implementation and finally a partial implementation of an autograd function.\n",
        "\n",
        "Your taks is to implement the backward passes for these functions and verify that your implementation is correct. This section will teach you how to write your own custom differentiable loss function and how to implement it in a way that fits in with standard pytorch classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NqO6_RIHDN4t"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def compare(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "w6lySAvoADfT"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    # Conv2d( in_channels, out_channels, kernel_size) B,3,32,32\n",
        "    nn.Conv2d(3, 32, 3, stride= 2, padding=1),  nn.BatchNorm2d(32), nn.Tanh(), # B,32,16,16\n",
        "    nn.Conv2d(32, 64, 3, stride= 2, padding=1), nn.BatchNorm2d(64), nn.Tanh(), # B,32,8,8\n",
        "    nn.Conv2d(64, 128, 3, stride= 2, padding=1), nn.BatchNorm2d(128), nn.Tanh(), # B,32,4,4\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(4*4*128, 64), nn.BatchNorm1d(64), nn.Tanh(),\n",
        "    nn.Linear(64, 10)\n",
        ")\n",
        "\n",
        "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2W5wMnY7Cluv"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "x = batch[0]\n",
        "y = batch[1]\n",
        "\n",
        "logits = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfBB_PLPTnoV"
      },
      "source": [
        "### **Exercise 1: Writing backward pass**\n",
        "\n",
        "*4 Points*\n",
        "\n",
        "Backprop through the loss function manually, backpropagating through exactly all of the variables as they are defined in the forward pass below, one by one starting from the end and propagating to the beginning.\n",
        "\n",
        "I have included notes and clues that will help you in backpropogation. Keep in mind that a derrivative for a variable should be the same shape as the orignal variable. Remember that if there is a broadcast operation in the forward pass you need to have a summing operation in the backward pass and if there is a summation in the forward pass you need to have a broadcast operation in the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAB9HnAsB54-",
        "outputId": "e9d5f886-fde1-4d99-8fd5-e0be545e298d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(2.3700, grad_fn=<NegBackward0>)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), y].mean()\n",
        "\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, norm_logits, logit_maxes, logits]:\n",
        "  t.retain_grad()\n",
        "\n",
        "\n",
        "optim.zero_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Mo-J0l3C_ZR",
        "outputId": "77cfa695-9695-4f9d-a9c4-f23354817909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "counts          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "norm_logits     | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "logit_maxes     | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "logits          | exact: False | approximate: True  | maxdiff: 3.259629011154175e-09\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# Write your answer here\n",
        "\n",
        "# You should consider which parts of the logprops is affecting the loss\n",
        "# Only backpropagate the gradients to those elements\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), y] = -1.0/n * 1\n",
        "\n",
        "# d/dx(log(x))= 1/x\n",
        "dprobs = (1/probs) * dlogprobs\n",
        "\n",
        "# Broadcasing in forward means summation in backward\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "\n",
        "# d/dx(1/x) = -x**-2\n",
        "dcounts_sum = -1 * dcounts_sum_inv / counts_sum**2\n",
        "\n",
        "# Remember that there are counts is used both for counts_sum and probs\n",
        "# so it should have 2 additive contributions\n",
        "# Remember that summation in forward pass is broadcasting in backward pass\n",
        "dcounts = dcounts_sum + dprobs * counts_sum_inv\n",
        "\n",
        "# d/dx(e^x)= e^x\n",
        "dnorm_logits = dcounts * counts\n",
        "\n",
        "# d/dx(-x)= -1 and broadcasing in forward means summation in backward\n",
        "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
        "\n",
        "# Remember that there are counts is used both for logit_maxes and norm_logits\n",
        "# so it should have 2 additive contributions\n",
        "# For the first component remember d/dx(x)= 1\n",
        "# For the second component remember to only affect the indices have max values\n",
        "dlogits = dnorm_logits + torch.zeros_like(logits).scatter_(1, logits.argmax(1, keepdim=True), dlogit_maxes)\n",
        "# ----------------------\n",
        "\n",
        "#YOU WILL USE THE COMPARE FUNCTIONS BELOW TO SEE IF YOU HAVE THE CORRECT SOLUTION OR NOT\n",
        "#ALSO THEY WILL BE USED IN THE GRADING PROCESS, RUN THEM ALL AND KEEP THEIR PRINTOUTS\n",
        "compare('logprobs', dlogprobs, logprobs)\n",
        "compare('probs', dprobs, probs)\n",
        "compare('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "compare('counts_sum', dcounts_sum, counts_sum)\n",
        "compare('counts', dcounts, counts)\n",
        "compare('norm_logits', dnorm_logits, norm_logits)\n",
        "compare('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "compare('logits', dlogits, logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI24jdZ8Rrj1"
      },
      "source": [
        "### **Exercise 2: Simplifying Cross-Entropy Expression**\n",
        "\n",
        "*2 points*\n",
        "\n",
        "Backprop through cross_entropy function in one step. Look at the mathematical expression for cross entropy $-\\sum_{c=1}^{10}y_{c}\\log(p_c)$, calculate the derivative analytically (with pen and paper) and finally write down the simplified expression for the backpropagation calculation.\n",
        "\n",
        "Remember that in order to convert the logits into propabilities $p$ we need to take the softmax. Since the labels are one-hot only the labels that are correct are going to affect the final loss $L$ and the expression for cross entropy can be simplified to\n",
        "$L=-log(\\frac{e^{logit_y}}{\\sum_{j}e^{logit_j}})$.\n",
        "\n",
        "Take the derivative of the loss with respect to the each dimension of the logit $logit_i$. Remember that you can simplify the experssion by considering two cases. One where $i=y$ and one where $i\\neq y$.\n",
        "\n",
        "Finally remember that we are calculating the loss over the batches and we need to average the losses over the batch dimension (i.e. divide by batch_size)\n",
        "\n",
        "Note: Your answer don't have to produce the exactly same result as the Pytorch implementation you can get an approximately correct version that has less max difference than 1e-8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEvmtpMPCG-J",
        "outputId": "09324b04-bbdd-4642-cb18-2ade6b351b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.370008707046509 diff: 2.384185791015625e-07\n"
          ]
        }
      ],
      "source": [
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum)\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, y)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nGWgdUYE5jV",
        "outputId": "2ae4e205-c4ea-4ee0-c15c-f3f2bacd5dbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 7.450580596923828e-09\n"
          ]
        }
      ],
      "source": [
        "# ----------------------\n",
        "# Write your answer here\n",
        "\n",
        "n = logits.shape[0]\n",
        "probs = F.softmax(logits, dim=1)\n",
        "one_hot_y = torch.zeros_like(logits)\n",
        "one_hot_y[range(n), y] = 1.0\n",
        "\n",
        "dlogits = (probs - one_hot_y) / n\n",
        "\n",
        "# ----------------------\n",
        "#THE CODE BELOW AND IT'S OUTPUT WILL BE USED IN THE GRADING PROCESS\n",
        "#USE IT TO TEST YOUR SOLUTION\n",
        "compare('logits', dlogits, logits) # It is ok to get approximate correctness, 1e-8 is OK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY5lvvVnRVO9"
      },
      "source": [
        "### **Exercise 3: Defining new Autograd function**\n",
        "\n",
        "*2 points*\n",
        "\n",
        "Now use your solution from the previous step to write a custom autograd Function. You can take a look at the [documententation](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html) for autograd functions in order to get a clearer idea.\n",
        "\n",
        "For the forward pass we have already provided the implementation steps. You don't need to backpropagate the loss with respect to the labels they are return with None in the template\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DiiaAPn2FNqb"
      },
      "outputs": [],
      "source": [
        "class CustomCrossEntropy(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    We can implement our own custom autograd Functions by subclassing\n",
        "    torch.autograd.Function and implementing the forward and backward passes\n",
        "    which operate on Tensors.\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, logits, y):\n",
        "        \"\"\"CustomCrossEntropy.apply\n",
        "        Here we save the elements necessary for calculating the backward pass.\n",
        "        We than use the efficient expression for the backward pass.\n",
        "        \"\"\"\n",
        "        ctx.save_for_backward(logits, y)\n",
        "        return F.cross_entropy(logits, y)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        \"\"\"\n",
        "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
        "        with respect to the output, and we need to compute the gradient of the loss\n",
        "        with respect to the input.\n",
        "        We first load the tensors that we have saved before\n",
        "        Calculate the local derivative and backpropagate it by multiplying the upstream\n",
        "        derivative with the local derivative. You don't need to backpropagate the gradient\n",
        "        for y and return None since we expect it to be a fixed value.\n",
        "        \"\"\"\n",
        "        logits, y, = ctx.saved_tensors\n",
        "\n",
        "        # ----------------------\n",
        "        # Write your answer here\n",
        "\n",
        "        n = logits.shape[0]\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        one_hot_y = torch.zeros_like(logits)\n",
        "        one_hot_y[range(n), y] = 1.0\n",
        "        dlogits = (probs - one_hot_y) / n\n",
        "\n",
        "        dlogits_new = grad_output * dlogits\n",
        "\n",
        "        return dlogits_new, None\n",
        "        # ----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gVr_fYdV4Cr"
      },
      "source": [
        "Below is a check for you to make sure that your code has been implemented correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb7roiWJKvgn",
        "outputId": "b43795c0-09d2-4e05-fc88-4e5577017868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pytorch cross entropy loss:  tensor(2.3218, grad_fn=<NllLossBackward0>)\n",
            "custom cross entropy loss:  tensor(2.3218, grad_fn=<CustomCrossEntropyBackward>)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#THE CODE BELOW AND IT'S OUTPUT WILL BE USED IN THE GRADING PROCESS\n",
        "#USE IT TO TEST YOUR SOLUTION\n",
        "batch = next(iter(dataloader))\n",
        "x = batch[0]\n",
        "y = batch[1]\n",
        "\n",
        "# Calculate loss and gradients using Pytorch CrossEntropy method\n",
        "logits = model(x)\n",
        "loss = F.cross_entropy(logits, y)\n",
        "\n",
        "logits.retain_grad()\n",
        "\n",
        "optim.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "pytorch_logits_grad = logits.grad\n",
        "\n",
        "print(\"pytorch cross entropy loss: \",loss)\n",
        "\n",
        "# Calculate loss using new CustomCrossEntropy method\n",
        "logits = model(x)\n",
        "loss_fn = CustomCrossEntropy.apply\n",
        "loss = loss_fn(logits, y)\n",
        "\n",
        "logits.retain_grad()\n",
        "\n",
        "optim.zero_grad()\n",
        "loss.backward()\n",
        "\n",
        "custom_logits_grad = logits.grad\n",
        "print(\"custom cross entropy loss: \",loss)\n",
        "\n",
        "torch.allclose(custom_logits_grad, pytorch_logits_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR8rqoR0q2RN"
      },
      "source": [
        "## Exercise 3b â€” Debug Softmax + Cross-Entropy (Forward & Backward)\n",
        "\n",
        "*2 points*\n",
        "\n",
        "**Goal:** Fix a deliberately wrong implementation of **softmax + cross-entropy** so that:\n",
        "1) itâ€™s numerically stable  \n",
        "2) the reduction in forward matches the scaling in backward  \n",
        "3) your manual gradient w.r.t. logits matches PyTorch autograd.\n",
        "\n",
        "**Whatâ€™s wrong down below?**\n",
        "\n",
        "The forward and backward procedures (`almost_correct_ce_forward()` & `almost_correct_ce_backward()`) of a loss function is designed by some beginner PyTorch enthusiast ðŸ˜Ž However he has done a little mistake in his definitions.   \n",
        "The procedure is as follows:\n",
        "- computed the loss via `softmax` + `-log(...)` (unstable),  \n",
        "- took a **sum** in the forward, but divided by **N** in the backward (mismatch),  \n",
        "- skipped using `log_softmax`.\n",
        "\n",
        "**What is **your** task?**\n",
        "- Replace the wrong forward/backward with a correct, stable formulation using `log_softmax`.\n",
        "- Ensure the forward reduction (e.g., `\"mean\"`) matches the gradient scaling.\n",
        "- Pass the unit tests that compare your manual gradient to `torch.nn.functional.cross_entropy`.\n",
        "\n",
        "### Hints\n",
        "\n",
        "- Correct logits-gradient for Cross-Entropy with softmax (mean reduction) is:\n",
        "\n",
        "    dL/dz = (p âˆ’ one_hot(y)) / N\n",
        "\n",
        "    where:\n",
        "    \n",
        "    â€¢ p = softmax(z)\n",
        "\n",
        "    â€¢ z = logits\n",
        "\n",
        "    â€¢ y = target class index\n",
        "\n",
        "    â€¢ N = batch size\n",
        "\n",
        "- Use the stable version of log-softmax to compute the loss:\n",
        "\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "\n",
        "- Don't forget:\n",
        "\n",
        "    â€¢ If you use \"sum\" reduction â†’ do NOT divide the gradient by N  \n",
        "\n",
        "    â€¢ If you use \"mean\" reduction â†’ do divide the gradient by N  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sGy6pfNmrYCK"
      },
      "outputs": [],
      "source": [
        "# Wrongly Written Reference (DO NOT MODIFY HERE)\n",
        "# ------------------------------------------------------------\n",
        "def almost_correct_ce_forward(logits: torch.Tensor, targets: torch.Tensor, reduction: str = \"mean\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "      - uses softmax + log, not log_softmax (numerical issues)\n",
        "      - uses SUM reduction regardless, then the \"reduction\" arg is half-respected\n",
        "    \"\"\"\n",
        "    N, C = logits.shape\n",
        "    probs = torch.softmax(logits, dim=1)  # unstable when logits have large magnitude\n",
        "    nll = -torch.log(probs[torch.arange(N), targets] + 1e-12)  # add eps, but still poor stability\n",
        "    if reduction == \"none\":\n",
        "        return nll\n",
        "    elif reduction == \"mean\":\n",
        "        return nll.sum()\n",
        "    elif reduction == \"sum\":\n",
        "        return nll.sum()\n",
        "    else:\n",
        "        raise ValueError(\"reduction must be 'none' | 'mean' | 'sum'\")\n",
        "\n",
        "def almost_correct_ce_backward(logits: torch.Tensor, targets: torch.Tensor, reduction: str = \"mean\") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "      - gradient formula assumes mean reduction (divides by N),\n",
        "        even if forward did .sum()\n",
        "    \"\"\"\n",
        "    N, C = logits.shape\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    grad = probs.clone()\n",
        "    grad[torch.arange(N), targets] -= 1.0\n",
        "    if reduction == \"none\":\n",
        "        grad = grad / N\n",
        "    elif reduction == \"mean\":\n",
        "        grad = grad / N\n",
        "    elif reduction == \"sum\":\n",
        "        grad = grad / N\n",
        "    return grad\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHKHKdcErYZo"
      },
      "source": [
        "Now please implement your fix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zcbCCCVgrYlH"
      },
      "outputs": [],
      "source": [
        "# Your fix: Implement a numerically stable CE\n",
        "# ------------------------------------------------------------\n",
        "def my_ce_forward(logits: torch.Tensor, targets: torch.Tensor, reduction: str = \"mean\") -> torch.Tensor:\n",
        "    # ======= YOUR CODE STARTS HERE =======\n",
        "  log_probs = F.log_softmax(logits, dim=1)\n",
        "  loss = F.nll_loss(log_probs, targets, reduction=reduction)\n",
        "\n",
        "  return loss\n",
        "    # pass\n",
        "    # ======= YOUR CODE ENDS HERE =======\n",
        "\n",
        "\n",
        "def my_ce_backward(logits: torch.Tensor, targets: torch.Tensor, reduction: str = \"mean\") -> torch.Tensor:\n",
        "    # ======= YOUR CODE STARTS HERE =======\n",
        "    N = logits.shape[0]\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    grad = probs.clone()\n",
        "    grad[torch.arange(N), targets] -= 1\n",
        "\n",
        "    if reduction == \"mean\":\n",
        "        grad = grad / N\n",
        "\n",
        "    return grad\n",
        "    # pass\n",
        "    # ======= YOUR CODE ENDS HERE =======\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxDWo-FgyHa1"
      },
      "source": [
        "Now test your fix please:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anW5nIuwyGN0",
        "outputId": "9ae236a8-d8ff-45cd-8e2a-45454d41f616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[reduction=none] mean |Î”grad| vs PyTorch autograd: 6.821938e-09\n",
            "[reduction=none] ref_loss=1.746523  my_loss=1.746523\n",
            "[reduction=mean] mean |Î”grad| vs PyTorch autograd: 8.527422e-10\n",
            "[reduction=mean] ref_loss=1.746523  my_loss=1.746523\n",
            "[reduction=sum] mean |Î”grad| vs PyTorch autograd: 6.821938e-09\n",
            "[reduction=sum] ref_loss=13.972184  my_loss=13.972184\n",
            "All reductions matched autograd.\n"
          ]
        }
      ],
      "source": [
        "# Unit tests: Compare to PyTorch's F.cross_entropy\n",
        "# ------------------------------------------------------------\n",
        "def check_grad_match(reduction: str = \"mean\", seed: int = 7, verbose=True):\n",
        "    torch.manual_seed(seed)\n",
        "    N, C = 8, 5\n",
        "    logits = torch.randn(N, C, requires_grad=True)\n",
        "    targets = torch.randint(0, C, (N,))\n",
        "\n",
        "    # PyTorch reference\n",
        "    ref_loss = F.cross_entropy(logits, targets, reduction=reduction)\n",
        "    # Provide a gradient tensor for non-scalar outputs\n",
        "    grad_tensor = torch.ones_like(ref_loss) if reduction == \"none\" else None\n",
        "    ref_loss.backward(gradient=grad_tensor)\n",
        "    ref_grad = logits.grad.detach().clone()\n",
        "\n",
        "    # Your manual grad (ensure fresh graph/grad)\n",
        "    logits2 = logits.detach().clone().requires_grad_(True)\n",
        "    my_loss = my_ce_forward(logits2, targets, reduction=reduction)\n",
        "    # Backward using our manual gradient injection (stop autograd, compare tensors)\n",
        "    manual_grad = my_ce_backward(logits2.detach(), targets, reduction=reduction)\n",
        "    # Check shape & finite\n",
        "    assert manual_grad.shape == logits2.shape and torch.isfinite(manual_grad).all(), \"Manual grad invalid.\"\n",
        "    # Compare with autograd's grad for the SAME forward loss\n",
        "    # (Call backward on logits2 with manual grad like a hook equivalence)\n",
        "    logits2.backward(manual_grad, retain_graph=False)  # gradient injection\n",
        "    my_grad = logits2.grad.detach().clone()\n",
        "\n",
        "    # Because CE is convex in logits, the true grad of CE wrt logits equals (softmax - one_hot)/reduction\n",
        "    # We compare derived grads numerically:\n",
        "    mae = (my_grad - ref_grad).abs().mean().item()\n",
        "    if verbose:\n",
        "        print(f\"[reduction={reduction}] mean |Î”grad| vs PyTorch autograd: {mae:.6e}\")\n",
        "    assert mae < 1e-6, f\"Gradient mismatch too large (MAE={mae}); fix your forward/backward.\"\n",
        "\n",
        "    # Compare loss values too\n",
        "    with torch.no_grad():\n",
        "        l_ref = ref_loss.item() if reduction != \"none\" else ref_loss.mean().item()\n",
        "        l_stu = my_ce_forward(logits.detach(), targets, reduction=reduction).item() if reduction != \"none\" else my_ce_forward(logits.detach(), targets, reduction=reduction).mean().item()\n",
        "    if verbose:\n",
        "        print(f\"[reduction={reduction}] ref_loss={l_ref:.6f}  my_loss={l_stu:.6f}\")\n",
        "    assert abs(l_ref - l_stu) < 1e-6, \"Loss value mismatch; check your forward reduction/stability.\"\n",
        "\n",
        "# Expect these to PASS after you fix student_ce_forward/backward:\n",
        "for red in [\"none\", \"mean\", \"sum\"]:\n",
        "    check_grad_match(reduction=red)\n",
        "print(\"All reductions matched autograd.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fp6JXnpZzYsF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr-ZHCOczxtt"
      },
      "source": [
        "***Expected output:***\n",
        "\n",
        "[reduction=none] mean |Î”grad| vs PyTorch autograd: 6.821938e-09\n",
        "\n",
        "[reduction=none] ref_loss=1.746523  my_loss=1.746523\n",
        "\n",
        "[reduction=mean] mean |Î”grad| vs PyTorch autograd: 8.527422e-10\n",
        "\n",
        "[reduction=mean] ref_loss=1.746523  my_loss=1.746523\n",
        "\n",
        "[reduction=sum] mean |Î”grad| vs PyTorch autograd: 6.821938e-09\n",
        "\n",
        "[reduction=sum] ref_loss=13.972184  my_loss=13.972184\n",
        "\n",
        "All reductions matched autograd."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQsQxafxB6S5"
      },
      "source": [
        "## Architecture Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULbDrbvuUYbn"
      },
      "source": [
        "### **Exercise 4: Convolutional Architecture**\n",
        "\n",
        "*5 points*\n",
        "\n",
        "Below you find the model definition for a Convolutional neural network. Your task is to optimize the network architecture below to improve the model performance. **The minimum criteria for success is %85 validation accuracy.** Please share your training setup including your architecture and any other modifications you have made on the discussion section opened on Moodle. **Three students who has the highest accuracy among you will get 2 extra points to their final course score.**\n",
        "\n",
        "You are free to use other components like MaxPool2d, Dropout and ReLU from torch.nn library. Please remember the architectural innovations and regularization methods mentioned in the class and try to emulate them here. You can also change augmentation, learning rate schedulers and optimization methods as you like. **Don't use predefined models (such as ResNet, VGG etc.) right away but you can reimplement them yourselves**.\n",
        "\n",
        "You can define blocks and bottleneck layers to define your architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8KuMxXYAL2jN"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "\n",
        "    nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "    nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),\n",
        "    nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "    nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "    nn.Conv2d(128, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),\n",
        "    nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "    nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "    nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "    nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "    nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "    nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "    nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "    nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
        "    nn.MaxPool2d(2, stride=2),\n",
        "\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "    nn.Flatten(),\n",
        "\n",
        "    nn.Linear(512, 256),\n",
        "    nn.BatchNorm1d(256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "\n",
        "    nn.Linear(256, 10)\n",
        ")\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BqfvcvXjAKWl"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "EPOCH_N = 30\n",
        "losses = []\n",
        "steps = []\n",
        "losses_t = []\n",
        "\n",
        "lre = torch.linspace(-1, 4, 1000)\n",
        "lrs = 10**lre\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=EPOCH_N*len(dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvGLf457AL5b",
        "outputId": "0f57a464-f60d-4691-9094-24cc6d6a4dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.0492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.3139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.4741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.1033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.2621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCH_N):\n",
        "  for batch in dataloader:\n",
        "    i +=1\n",
        "    x = batch[0].to('cuda')\n",
        "    y = batch[1].to('cuda')\n",
        "    model = model.to('cuda')\n",
        "\n",
        "    logits = model(x)\n",
        "    loss = F.cross_entropy(logits,y)\n",
        "    optim.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # optim.lr =  0.01 if i < 10000 else 0.001\n",
        "    # optim.param_groups[0]['lr'] = 0.01 if i < 10000 else 0.001\n",
        "    # optim.lr = lrs[i]\n",
        "    optim.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    losses_t.append(loss.detach())\n",
        "    steps.append(i)\n",
        "\n",
        "    if i % 1000 ==0:\n",
        "      print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "2QXHZi7ub5Tl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "([<matplotlib.lines.Line2D at 0x228225ab830>], tensor(179.1651))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(steps, losses) , lrs[650]# , logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BofyymADK1l1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.99902\n"
          ]
        }
      ],
      "source": [
        "total_correct = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Loop over training dataset\n",
        "for x_batch, y_batch in dataloader:\n",
        "    logits = model(x_batch.to(\"cuda\")) # Forward pass on the mini-batch\n",
        "    loss = F.cross_entropy(logits.cpu(), y_batch) # Compute loss\n",
        "\n",
        "    # Calculate predictions for the batch\n",
        "    pred_labels = torch.max(logits, dim=1).indices\n",
        "\n",
        "    # Update total correct predictions and total predictions\n",
        "    total_correct += (y_batch == pred_labels.cpu()).sum().item()\n",
        "    total_predictions += y_batch.size(0)\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = total_correct / total_predictions\n",
        "print(f\"Training Accuracy: {overall_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "RtEixUxLb54K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testting Accuracy: 0.8986\n"
          ]
        }
      ],
      "source": [
        "total_correct = 0\n",
        "total_predictions = 0\n",
        "\n",
        "# Loop over testing dataset\n",
        "for x_batch, y_batch in dataloader_test:\n",
        "    logits = model(x_batch.to(\"cuda\")) # Forward pass on the mini-batch\n",
        "    loss = F.cross_entropy(logits.cpu(), y_batch) # Compute loss\n",
        "\n",
        "    # Calculate predictions for the batch\n",
        "    pred_labels = torch.max(logits, dim=1).indices\n",
        "\n",
        "    # Update total correct predictions and total predictions\n",
        "    total_correct += (y_batch == pred_labels.cpu()).sum().item()\n",
        "    total_predictions += y_batch.size(0)\n",
        "\n",
        "# Calculate overall accuracy and print it out.\n",
        "overall_accuracy = total_correct / total_predictions\n",
        "#DON'T FORGET TO PRINT OUT YOUR TESTING ACCURACY\n",
        "print(f\"Testting Accuracy: {overall_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
